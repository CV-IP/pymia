{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% raw\n"
    },
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _example-evaluation2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging the training progress\n",
    "=============================\n",
    "\n",
    "This example shows how to use the `pymia.evaluation` package to log the training progress in deep learning projects.\n",
    "The Jupyter notebook can be found at `./examples/evaluation/logging_torch.ipynb`.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "To be able to run this example:\n",
    "\n",
    "- Get the example data by executing `./examples/example-data/pull_example_data.py`.\n",
    "- Install torch (`pip install torch`)\n",
    "- Install tensorboard (`pip install tensorboard`)\n",
    "- You should have a basic understanding of the `pymia.data` package, see the example \"Data extraction and assembling\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymia.data.assembler as assm\n",
    "import pymia.data.backends.pytorch as pymia_torch\n",
    "import pymia.data.conversion as conv\n",
    "import pymia.data.definition as defs\n",
    "import pymia.data.extraction as extr\n",
    "import pymia.data.transformation as tfm\n",
    "import pymia.evaluation.metric as metric\n",
    "import pymia.evaluation.evaluator as eval_\n",
    "import pymia.evaluation.writer as writer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "import torch.utils.tensorboard as tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to log predictions of segmentations of a neural network against a reference ground truth. Common metrics in medical image segmentation are the Dice coefficient, an overlap-based metric, and the Hausdorff distance, a distance-based metric. Further, we also evaluate the volume similarity, a metric that does not consider the spatial overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics = [metric.DiceCoefficient(), metric.HausdorffDistance(percentile=95, metric='HDRFDST95'), metric.VolumeSimilarity()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define the labels we want to log during the training. In the provided example data, we have five labels for different brain structures. But we are only interested in three of them: white matter, grey matter, and the thalamus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = {1: 'WHITEMATTER',\n",
    "          2: 'GREYMATTER',\n",
    "          5: 'THALAMUS'\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the metrics and labels, we can initialize the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = eval_.SegmentationEvaluator(metrics, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The evaluator will return results for all subjects in the dataset. However, we would like to log only statistics like the mean and the standard deviation of the metrics among all subjects. Therefore, we initialize a statistics aggregator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "functions = {'MEAN': np.mean, 'STD': np.std}\n",
    "statistics_aggregator = writer.StatisticsAggregator(functions=functions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [TensorBoard](https://www.tensorflow.org/tensorboard) is commonly used to visualize the training in deep learning. PyTorch provides a module to log to the TensorBoard, which we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_dir = '../example-data/log'\n",
    "tb = tensorboard.SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize the data handling, please refer to the above mentioned example to understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file = '../example-data/example-dataset.h5'\n",
    "transform = tfm.Permute(permutation=(2, 0, 1), entries=(defs.KEY_IMAGES,))\n",
    "dataset = extr.PymiaDatasource(hdf_file, extr.SliceIndexing(), extr.DataExtractor(categories=(defs.KEY_IMAGES,)), transform)\n",
    "pytorch_dataset = pymia_torch.PytorchDatasetAdapter(dataset)\n",
    "loader = torch_data.dataloader.DataLoader(pytorch_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "assembler = assm.SubjectAssembler(dataset)\n",
    "direct_extractor = extr.ComposeExtractor([\n",
    "    extr.SubjectExtractor(),  # extraction of the subject name for evaluation\n",
    "    extr.ImagePropertiesExtractor(),  # extraction of image properties (origin, spacing, etc.) for evaluation in physical space\n",
    "    extr.DataExtractor(categories=(defs.KEY_LABELS,))  # extraction of \"labels\" entries for evaluation\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now define a dummy network, which will actually just return a random prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DummyNetwork(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.randint(0, 5, (*x.size()[:-1], 1))\n",
    "\n",
    "dummy_network = DummyNetwork()\n",
    "torch.manual_seed(0)  # set seed for reproducibility"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the training loop. We will loop over the samples in our dataset, feed them to the \"neural network\", and assemble them to back to entire volumetric predictions. As soon as a prediction is fully assembled, it will be evaluated against its reference. We do this evaluation in the physical space, as the spacing might be important for metrics like the Hausdorff distance (distances in mm rather than voxels). At the end of each epoch, we can calculate the mean and standard deviation of the metrics among all subjects in the dataset, and log them to the TensorBoard.\n",
    "Note that this example is just for illustration because usually you would want to log the performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-e14cf7b5f67d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mvalidation_batch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvalidation_dataset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0massembled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msubject_assembler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_assembled_subject\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdefs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mKEY_SUBJECT_INDEX\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "nb_batches = len(loader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i, batch in enumerate(loader):\n",
    "        # get the data from batch and predict\n",
    "        x, sample_indices = batch[defs.KEY_IMAGES], batch[defs.KEY_SAMPLE_INDEX]\n",
    "        prediction = dummy_network(x)\n",
    "\n",
    "        # translate the prediction to numpy and back to (B)HWC (channel last)\n",
    "        numpy_prediction = prediction.numpy().transpose((0, 2, 3, 1))\n",
    "\n",
    "        # add the batch prediction to the assembler\n",
    "        is_last = i == nb_batches - 1\n",
    "        assembler.add_batch(numpy_prediction, sample_indices.numpy(), is_last)\n",
    "\n",
    "        # Process the subjects/images that are fully assembled\n",
    "        for subject_index in assembler.subjects_ready:\n",
    "            subject_prediction = assembler.get_assembled_subject(subject_index)\n",
    "\n",
    "            # Extract the target and image properties via direct extract\n",
    "            direct_sample = dataset.direct_extract(direct_extractor, subject_index)\n",
    "            reference, image_properties = direct_sample[defs.KEY_LABELS], direct_sample[defs.KEY_PROPERTIES]\n",
    "\n",
    "            # convert prediction and reference back to SimpleITK images, as we want to use the physical spacing for the Hausdorff distance metric\n",
    "            prediction_image = conv.NumpySimpleITKImageBridge.convert(subject_prediction, image_properties)\n",
    "            reference_image = conv.NumpySimpleITKImageBridge.convert(reference, image_properties)\n",
    "\n",
    "            # evaluate the prediction against the reference\n",
    "            evaluator.evaluate(prediction_image, reference_image, direct_sample[defs.KEY_SUBJECT])\n",
    "\n",
    "    # calculate mean and standard deviation of each metric\n",
    "    results = statistics_aggregator.calculate(evaluator.results)\n",
    "    # log to TensorBoard into category train\n",
    "    for result in results:\n",
    "        tb.add_scalar(f'train/{result.metric}-{result.id_}', result.value, epoch)\n",
    "\n",
    "    # clear results such that the evaluator is ready for the next evaluation\n",
    "    evaluator.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can now start the TensorBoard and point the location to the log directory:\n",
    "\n",
    ".. code-block:: bash\n",
    "\n",
    "    cd <path_to_pymia>/examples/example-data/log\n",
    "    tensorboard --logdir=.\n",
    "\n",
    "Open a browser and type `localhost:6006` to see the logged training progress."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}