{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% raw\n"
    },
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _example-evaluation2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging the training progress\n",
    "=============================\n",
    "\n",
    "This example shows how to use the `pymia.evaluation` package to log the training progress in deep learning projects.\n",
    "The Jupyter notebook can be found at `./examples/evaluation/logging_torch.ipynb`.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "To be able to run this example:\n",
    "\n",
    "- Get the example data by executing `./examples/example-data/pull_example_data.py`.\n",
    "- Install torch (`pip install torch`)\n",
    "- You should have a basic understanding of the `pymia.data` package, see example :ref:`TODO(fabianbalsiger): title <example-data1>`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymia.data.definition as defs\n",
    "import pymia.evaluation.metric as metric\n",
    "import pymia.evaluation.evaluator as eval_\n",
    "import pymia.evaluation.writer as writer\n",
    "import torch.utils.tensorboard as tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the path to the dataset and the logging path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file = '../example-data/example-dataset.h5'\n",
    "log_path = '../example-data/log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to log predictions of segmentations of a neural network against a reference ground truth. Common metrics in medical image segmentation are the Dice coefficient, an overlap-based metric, and the Hausdorff distance, a distance-based metric. Further, we also evaluate the volume similarity, a metric that does not consider the spatial overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics = [metric.DiceCoefficient(), metric.HausdorffDistance(percentile=95), metric.VolumeSimilarity()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define the labels we want to evaluate. In the provided example data, we have TODO labels for different brain structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = {1: 'DUMMY-LABEL',  # todo(fabianbalsiger): adapt labels to example\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can initialize the evaluator with the metrics and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = eval_.SegmentationEvaluator(metrics, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are interested in logging the training progress, we will use the commonly used TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tb = tensorboard.SummaryWriter(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size_training = 10\n",
    "batch_size_testing = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the training loop. After each epoch of training, we will evaluate the predictions against the references, calculate the mean and standard deviation of the metrics, and log them to the TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-e14cf7b5f67d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mvalidation_batch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvalidation_dataset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0massembled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msubject_assembler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_assembled_subject\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdefs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mKEY_SUBJECT_INDEX\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "for epoch in epochs:\n",
    "    pass\n",
    "\n",
    "    for validation_batch in validation_dataset:\n",
    "        assembled = subject_assembler.get_assembled_subject(sample[defs.KEY_SUBJECT_INDEX])\n",
    "        prediction_image = pymia_conv.NumpySimpleITKImageBridge.convert(assembled, sample[defs.KEY_PROPERTIES])\n",
    "\n",
    "        # evaluate the \"prediction\" against the ground truth\n",
    "        evaluator.evaluate(prediction_image, label_image, sample[defs.KEY_SUBJECT])\n",
    "\n",
    "    # calculate mean and standard deviation of each metric on the validation dataset\n",
    "    results = writer.StatisticsAggregator(functions={'MEAN': np.mean, 'STD': np.std}).calculate(evaluator.results)\n",
    "    # log to TensorBoard\n",
    "    for result in results:\n",
    "        tb.add_scalar(f'valid/{result.metric}-{result.id_}', result.value, epoch)\n",
    "\n",
    "    # clear results such that the evaluator is ready for the next evaluation\n",
    "    evaluator.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}