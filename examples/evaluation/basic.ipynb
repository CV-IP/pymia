{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    },
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _example-evaluation1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basic evaluation of results\n",
    "===========================\n",
    "\n",
    "This example shows how to use the :mod:`pymia.evaluation` package to evaluate results.\n",
    "The Jupyter notebook can be found at `./examples/evaluation/basic.ipynb`.\n",
    "\n",
    ".. note::\n",
    "   To be able to run this example:\n",
    "\n",
    "    * Get the example data by executing `./examples/example-data/pull_example_data.py`.\n",
    "    * Install pandas (`pip install pandas`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pymia.evaluation.metric as metric\n",
    "import pymia.evaluation.evaluator as eval_\n",
    "import pymia.evaluation.writer as writer\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the paths to the data and the result CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../example-data'\n",
    "\n",
    "result_file = '../example-data/results.csv'\n",
    "result_summary_file = '../example-data/results_summary.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to evaluate segmentations against a reference ground truth. Common metrics in medical image segmentation are the Dice coefficient, an overlap-based metric, and the Hausdorff distance, a distance-based metric. Further, we also evaluate the volume similarity, a metric that does not consider the spatial overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [metric.DiceCoefficient(), metric.HausdorffDistance(percentile=95), metric.VolumeSimilarity()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define the labels we want to evaluate. In the provided example data, we have TODO labels for different brain structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {1: 'DUMMY-LABEL',  # todo(fabianbalsiger): adapt labels to example\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can initialize the evaluator with the metrics and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = eval_.SegmentationEvaluator(metrics, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop over the subjects of the example data. We will load the ground truth image as reference. An artificial segmentation (prediction) is created by eroding the ground truth. Both images, and the subject identifier are passed to the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Subject_1...\n",
      "Evaluating Subject_2...\n",
      "Evaluating Subject_3...\n",
      "Evaluating Subject_4...\n"
     ]
    }
   ],
   "source": [
    "# get subjects to evaluate\n",
    "subject_dirs = [subject for subject in glob.glob(os.path.join(data_dir, '*')) if os.path.isdir(subject)]\n",
    "\n",
    "for subject_dir in subject_dirs:\n",
    "    subject_id = os.path.basename(subject_dir)\n",
    "    print(f'Evaluating {subject_id}...')\n",
    "\n",
    "    # load ground truth image and create artificial prediction by erosion\n",
    "    ground_truth = sitk.ReadImage(os.path.join(subject_dir, f'{subject_id}_GT.mha'))\n",
    "    prediction = sitk.BinaryErode(ground_truth)  # todo(fabianbalsiger): handle multi label dummy data (BRATS)\n",
    "\n",
    "    # evaluate the \"prediction\" against the ground truth\n",
    "    evaluator.evaluate(prediction, ground_truth, subject_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we evaluated all subjects, we can use a CSV writer to write the evaluation results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.CSVWriter(result_file).write(evaluator.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we can use a console writer to display the results in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject-wise results...\n",
      "SUBJECT    LABEL        DICE   HDRFDST  VOLSMTY\n",
      "Subject_1  DUMMY-LABEL  0.642  6.708    0.642  \n",
      "Subject_2  DUMMY-LABEL  0.654  6.000    0.654  \n",
      "Subject_3  DUMMY-LABEL  0.641  6.164    0.641  \n",
      "Subject_4  DUMMY-LABEL  0.649  6.000    0.649  \n"
     ]
    }
   ],
   "source": [
    "print('\\nSubject-wise results...')\n",
    "writer.ConsoleWriter().write(evaluator.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also report statistics such as the mean and standard deviation among all subjects using dedicated statistics writers. Note that you can pass any functions that take a list of floats and return a scalar value to the writers. Again, we will write a CSV file and display the results in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated statistic results...\n",
      "LABEL        METRIC   STATISTIC  VALUE\n",
      "DUMMY-LABEL  DICE     MEAN       0.647\n",
      "DUMMY-LABEL  DICE     STD        0.005\n",
      "DUMMY-LABEL  HDRFDST  MEAN       6.218\n",
      "DUMMY-LABEL  HDRFDST  STD        0.291\n",
      "DUMMY-LABEL  VOLSMTY  MEAN       0.647\n",
      "DUMMY-LABEL  VOLSMTY  STD        0.005\n"
     ]
    }
   ],
   "source": [
    "functions = {'MEAN': np.mean, 'STD': np.std}\n",
    "writer.CSVStatisticsWriter(result_summary_file, functions=functions).write(evaluator.results)\n",
    "print('\\nAggregated statistic results...')\n",
    "writer.ConsoleStatisticsWriter(functions=functions).write(evaluator.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we clear the results in the evaluator such that the evaluator is ready for the next evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us have a look at the saved result CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT;LABEL;DICE;HDRFDST;VOLSMTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject_1;DUMMY-LABEL;0.6420209637040026;6.708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject_2;DUMMY-LABEL;0.6542393455768277;6.0;0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject_3;DUMMY-LABEL;0.6412505192785168;6.164...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject_4;DUMMY-LABEL;0.6492026923399021;6.0;0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  SUBJECT;LABEL;DICE;HDRFDST;VOLSMTY\n",
       "0  Subject_1;DUMMY-LABEL;0.6420209637040026;6.708...\n",
       "1  Subject_2;DUMMY-LABEL;0.6542393455768277;6.0;0...\n",
       "2  Subject_3;DUMMY-LABEL;0.6412505192785168;6.164...\n",
       "3  Subject_4;DUMMY-LABEL;0.6492026923399021;6.0;0..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also at the saved statistics CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL;METRIC;STATISTIC;VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUMMY-LABEL;DICE;MEAN;0.6466783802248124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUMMY-LABEL;DICE;STD;0.005354753771402503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUMMY-LABEL;HDRFDST;MEAN;6.218154483867086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUMMY-LABEL;HDRFDST;STD;0.29078310604924873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUMMY-LABEL;VOLSMTY;MEAN;0.6466783802248123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DUMMY-LABEL;VOLSMTY;STD;0.005354753771402463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   LABEL;METRIC;STATISTIC;VALUE\n",
       "0      DUMMY-LABEL;DICE;MEAN;0.6466783802248124\n",
       "1     DUMMY-LABEL;DICE;STD;0.005354753771402503\n",
       "2    DUMMY-LABEL;HDRFDST;MEAN;6.218154483867086\n",
       "3   DUMMY-LABEL;HDRFDST;STD;0.29078310604924873\n",
       "4   DUMMY-LABEL;VOLSMTY;MEAN;0.6466783802248123\n",
       "5  DUMMY-LABEL;VOLSMTY;STD;0.005354753771402463"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(result_summary_file)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}